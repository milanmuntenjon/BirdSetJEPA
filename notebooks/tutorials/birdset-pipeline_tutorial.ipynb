{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed02a1b-72ee-475e-9794-a6c0396fea10",
   "metadata": {},
   "source": [
    "# BirdSet Data Pipeline Tutorial\n",
    "\n",
    "This Jupyter notebook provides a comprehensive guide to setting up and configuring a data pipeline tailored for bird classification in audio files. The tutorial is structured to walk you through each component of the pipeline, ensuring a clear understanding of its functionality and configuration. Whether you are processing raw audio data or spectrograms, this notebook aims to provide you with the necessary knowledge to efficiently set up your data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909c001-7d00-4a0f-a16c-c984e7d91740",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Prerequisites\n",
    "Before initiating the installation process of the BirdSet pipeline, it's crucial to ensure that your computing environment meets the following prerequisites:\n",
    "- **Python**: You should have Python version 3.10 or higher installed on your system.\n",
    "\n",
    "### Installation Steps\n",
    "The BirdSet pipeline can be installed using either of the two methods: via Conda with Pip, or using Poetry. Select the method that best suits your preference and follow the corresponding steps below.\n",
    "\n",
    "#### Using Conda and Pip\n",
    "\n",
    "1. **Create a Conda Environment**: Begin by setting up a dedicated environment for BirdSet. This is a best practice to manage dependencies and avoid potential conflicts with other packages in your system.\n",
    "\n",
    "   ```bash\n",
    "   conda create -n birdset python=3.10\n",
    "   ```\n",
    "\n",
    "   After the environment is successfully created, activate it:\n",
    "\n",
    "   ```bash\n",
    "   conda activate birdset\n",
    "   ```\n",
    "\n",
    "2. **Install BirdSet**: Proceed with cloning the BirdSet repository and installing the package in editable mode. This approach is beneficial as it allows any modifications you make to the BirdSet code to be reflected immediately without the need for reinstallation.\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/DBD-research-group/BirdSet.git\n",
    "   cd BirdSet\n",
    "   pip install -e .\n",
    "   ```\n",
    "\n",
    "#### Using Poetry\n",
    "\n",
    "1. **Clone the Repository**: Start with cloning the BirdSet repository to your local machine and navigate to the cloned directory.\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/DBD-research-group/BirdSet.git\n",
    "   cd BirdSet\n",
    "   ```\n",
    "\n",
    "2. **Install Dependencies and Activate Environment**: Install all the necessary dependencies with Poetry and then activate the Poetry shell environment.\n",
    "\n",
    "   ```bash\n",
    "   poetry install\n",
    "   poetry shell\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4f398-f18d-42e4-a6b5-02329f1f5567",
   "metadata": {},
   "source": [
    "## Log in to Huggingface\n",
    "\n",
    "Our datasets are shared via HuggingFace Datasets in our [HuggingFace BirdSet repository](https://huggingface.co/datasets/DBD-research-group/birdset_v1). Huggingface is a central hub for sharing and utilizing datasets and models, particularly beneficial for machine learning and data science projects. For accessing private datasets hosted on HuggingFace, you need to be authenticated. Here's how you can log in to HuggingFace:\n",
    "\n",
    "1. **Install HuggingFace CLI**: If you haven't already, you need to install the HuggingFace CLI (Command Line Interface). This tool enables you to interact with HuggingFace services directly from your terminal. You can install it using pip:\n",
    "\n",
    "   ```bash\n",
    "   pip install huggingface_hub\n",
    "   ```\n",
    "\n",
    "2. **Login via CLI**: Once the HuggingFace CLI is installed, you can log in to your HuggingFace account directly from your terminal. This step is essential for accessing private datasets or contributing to the HuggingFace community. Use the following command:\n",
    "\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "\n",
    "   After executing this command, you'll be prompted to enter your HuggingFace credentials ([User Access Token](https://huggingface.co/docs/hub/security-tokens)). Once authenticated, your credentials will be saved locally, allowing seamless access to HuggingFace resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c29ea2",
   "metadata": {},
   "source": [
    "## TLDR;\n",
    "To get started with the default configuration, you can use the following code snippet to set up the BirdSet pipeline to load the [High Sierras](https://zenodo.org/records/7525805) test dataset (10,296 samples) including a train set (5,197 samples) with matching bird classes from [xeno-canto](https://xeno-canto.org/). The total size of the dataset is 6.2GB. The samples will be provided as spectrograms with a resolution of `128x1024` pixels in batches of size `32`, the labels are one-hot encoded for a multilabel classification task. Down below you find further information on how to configure the pipeline to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f345d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:25:54.876793900Z",
     "start_time": "2024-05-27T10:25:54.663332400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/workspace/data_birdset/HSN/DBD-research-group___bird_set/HSN/0.0.4/c6d7d1c30e4d8f29c4fb5e358bc605ccb896681b869398c54db64721f45ac91e_builder.lock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m      5\u001b[0m dm \u001b[38;5;241m=\u001b[39m BirdSetDataModule(\n\u001b[1;32m      6\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mDatasetConfig(\n\u001b[1;32m      7\u001b[0m         data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data_birdset/HSN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ),\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# prepare the data (download dataset, ...)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# setup the dataloaders\u001b[39;00m\n\u001b[1;32m     21\u001b[0m dm\u001b[38;5;241m.\u001b[39msetup(stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/birdset/datamodule/base_datamodule.py:120\u001b[0m, in \u001b[0;36mBaseDataModuleHF.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    118\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrepare Data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(dataset)\n\u001b[1;32m    122\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_splits(dataset)\n",
      "File \u001b[0;32m/workspace/birdset/datamodule/birdset_datamodule.py:78\u001b[0m, in \u001b[0;36mBirdSetDataModule._load_data\u001b[0;34m(self, decode)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, decode: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Loads the data.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    This method loads the data by calling the superclass's _load_data method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m        The loaded data.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/birdset/datamodule/base_datamodule.py:277\u001b[0m, in \u001b[0;36mBaseDataModuleHF._load_data\u001b[0;34m(self, decode)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_config\u001b[38;5;241m.\u001b[39mhf_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesc50\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# special esc50 case due to naming\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     dataset_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_config\u001b[38;5;241m.\u001b[39mhf_name\n\u001b[0;32m--> 277\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, IterableDataset \u001b[38;5;241m|\u001b[39m IterableDatasetDict):\n\u001b[1;32m    280\u001b[0m     log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable datasets not supported yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/datasets/load.py:2151\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2151\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2161\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2162\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/datasets/builder.py:859\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m     lock_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_builder.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# File locking only with local paths; no file locking on GCS or S3\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m FileLock(lock_path) \u001b[38;5;28;01mif\u001b[39;00m is_local \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext():\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Check if the data already exists\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     data_exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39mexists(posixpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME))\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_exists \u001b[38;5;129;01mand\u001b[39;00m download_mode \u001b[38;5;241m==\u001b[39m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/filelock/_api.py:376\u001b[0m, in \u001b[0;36mBaseFileLock.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Acquire the lock.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    :return: the lock object\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/filelock/_api.py:332\u001b[0m, in \u001b[0;36mBaseFileLock.acquire\u001b[0;34m(self, timeout, poll_interval, poll_intervall, blocking)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_locked:\n\u001b[1;32m    331\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to acquire lock \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lock_id, lock_filename)\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_locked:\n\u001b[1;32m    334\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLock \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m acquired on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lock_id, lock_filename)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/filelock/_unix.py:42\u001b[0m, in \u001b[0;36mUnixFileLock._acquire\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_file)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     41\u001b[0m     open_flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mO_CREAT\n\u001b[0;32m---> 42\u001b[0m fd \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopen_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mPermissionError\u001b[39;00m):  \u001b[38;5;66;03m# This locked is not owned by this UID\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     os\u001b[38;5;241m.\u001b[39mfchmod(fd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/workspace/data_birdset/HSN/DBD-research-group___bird_set/HSN/0.0.4/c6d7d1c30e4d8f29c4fb5e358bc605ccb896681b869398c54db64721f45ac91e_builder.lock'"
     ]
    }
   ],
   "source": [
    "from birdset.datamodule import DatasetConfig\n",
    "from birdset.datamodule.birdset_datamodule import BirdSetDataModule\n",
    "\n",
    "# initiate the data module\n",
    "dm = BirdSetDataModule(\n",
    "    dataset=DatasetConfig(\n",
    "        data_dir=\"../../data_birdset/HSN\",\n",
    "        hf_path=\"DBD-research-group/BirdSet\",\n",
    "        hf_name=\"HSN\",\n",
    "        n_workers=3,\n",
    "        val_split=0.2,\n",
    "        task=\"multilabel\",\n",
    "        classlimit=500,\n",
    "        eventlimit=5,\n",
    "        sample_rate=32000,\n",
    "    ),\n",
    ")\n",
    "# prepare the data (download dataset, ...)\n",
    "dm.prepare_data()\n",
    "# setup the dataloaders\n",
    "dm.setup(stage=\"fit\")\n",
    "# get the dataloaders\n",
    "train_loader = dm.train_dataloader()\n",
    "# get the first batch\n",
    "batch = next(iter(train_loader))\n",
    "# get shape of the batch\n",
    "print(batch[\"input_values\"].shape)\n",
    "print(batch[\"labels\"].shape)\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5584fa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "\n",
    "min_epochs = 1\n",
    "max_epochs = 5\n",
    "trainer = Trainer(\n",
    "    min_epochs=min_epochs,\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    fast_dev_run=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4481caa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:27:01.894487400Z",
     "start_time": "2024-05-27T10:27:01.330278300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n"
     ]
    }
   ],
   "source": [
    "from birdset.modules.base_module import BaseModule\n",
    "\n",
    "model = BaseModule(\n",
    "    len_trainset=dm.len_trainset,\n",
    "    task=dm.task,\n",
    "    batch_size=dm.train_batch_size,\n",
    "    num_epochs=max_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cc4646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:259: Found unsupported keys in the lr scheduler dict: {'warmup_ratio'}. HINT: remove them from the output of `configure_optimizers`.\n",
      "\n",
      "  | Name                  | Type                   | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss       | 0      | train\n",
      "1 | model                 | EfficientNetClassifier | 6.5 M  | train\n",
      "2 | train_metric          | MulticlassAccuracy     | 0      | train\n",
      "3 | valid_metric          | MulticlassAccuracy     | 0      | train\n",
      "4 | test_metric           | MulticlassAccuracy     | 0      | train\n",
      "5 | valid_metric_best     | MaxMetric              | 0      | train\n",
      "6 | valid_add_metrics     | MetricCollection       | 0      | train\n",
      "7 | test_add_metrics      | MetricCollection       | 0      | train\n",
      "8 | test_complete_metrics | MetricCollection       | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "26.158    Total estimated model params size (MB)\n",
      "479       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc597ff9ffe04f9d8066cee013b54c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b733dea4de104435815bfaf10ea79dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n",
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c79c1fe0cc397d6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa5935ac23044febab38d88c1ce141a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test/CrossEntropyLoss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.7179646492004395     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test/F1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/MulticlassAccuracy  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test/CrossEntropyLoss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.7179646492004395    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test/F1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/MulticlassAccuracy \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/CrossEntropyLoss': 1.7179646492004395,\n",
       "  'test/MulticlassAccuracy': 0.0,\n",
       "  'test/F1': 0.0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be30bd-772f-407f-9925-ef247dbc8219",
   "metadata": {},
   "source": [
    "## Configuration of BirdSet Data Pipeline\n",
    "\n",
    "The BirdSet Data Pipeline offers a robust and flexible configuration system, primarily designed to streamline the process of setting up your data processing environment. While this notebook presents hardcoded configurations for simplicity, it's important to note that these settings can be dynamically managed using advanced configuration tools like Hydra. Hydra is a powerful utility that enables flexible and scalable configuration management, allowing you to adapt the pipeline settings to various environments or use cases seamlessly. For an in-depth understanding of Hydra, consider visiting [Hydra's official documentation](https://hydra.cc/docs/intro).\n",
    "\n",
    "Tipp! Detailed information is provided in the docstrings of the classes and functions. You can access them by hovering over the class or function name in your IDE or by opening the source file in a text editor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad76228",
   "metadata": {},
   "source": [
    "### BirdSetDataModule\n",
    "The `BirdSetDataModule` is a [PyTorch Lightning DataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html#lightningdatamodule) that encapsulates the entire data pipeline. It inherits from `BaseDataModuleHF` which is a base class for all DataModules that use [HuggingFace datasets libary](https://huggingface.co/docs/datasets/index).\n",
    "\n",
    "To initialize the `BirdSetDataModule`, you need to provide the following parameters:\n",
    "\n",
    "```python\n",
    "from src.datamodule.birdset_datamodule import BirdSetDataModule\n",
    "\n",
    "data_module = BirdSetDataModule(\n",
    "    dataset=dataset_config, #dataset (DatasetConfig): The configuration for the dataset.\n",
    "    loaders=loaders_config, #loaders (LoadersConfig): The configuration for the loaders.\n",
    "    transforms=transforms, #transforms (BirdSetTransformsWrapper): The transforms to be applied to the data.\n",
    "    mapper=mapper #mapper (XCEventMapping): The mapping for the events.\n",
    ")\n",
    "```\n",
    "\n",
    "We will now walk through each of these parameters to understand their functionality and configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158f248-890f-47d0-8b16-68758a314685",
   "metadata": {},
   "source": [
    "### 1. Dataset Configuration\n",
    "\n",
    "Configuring the dataset is the first step in configuring the BirdSet data pipeline, here you specify which dataset you want to load, how many classes it has, and how the data is split into train, validation, and test sets. The `DatasetConfig` class is used to configure the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cca9fe-2ac0-4418-93d1-e8ad5fd37786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:55:39.449314Z",
     "start_time": "2024-05-23T08:55:39.380976700Z"
    }
   },
   "outputs": [],
   "source": [
    "from birdset.datamodule import DatasetConfig\n",
    "\n",
    "dataset_config = DatasetConfig(\n",
    "    data_dir=\"../../data_birdset/HSN\",\n",
    "    hf_path=\"DBD-research-group/BirdSet\",\n",
    "    hf_name=\"HSN\",\n",
    "    n_workers=1,\n",
    "    val_split=0.2,\n",
    "    task=\"multilabel\",\n",
    "    subset=None,\n",
    "    sample_rate=32000,\n",
    "    class_weights_sampler=None,\n",
    "    classlimit=500,\n",
    "    eventlimit=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea4694-e2e8-4b71-a2ad-6b004850f7e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here's a brief overview of the parameters used in the `DatasetConfig` class:\n",
    "\n",
    "- `data_dir`: Specifies the directory where the dataset files are stored. **Important**: The dataset uses a lot of disk space, so make sure you have enough storage available.\n",
    "- `hf_path`: The path to the dataset stored on HuggingFace.\n",
    "- `hf_name`: The name of the dataset on HuggingFace.\n",
    "- `seed`: A seed value for ensuring reproducibility across runs.\n",
    "- `n_classes`: The total number of distinct classes in the dataset.\n",
    "- `n_workers`: The number of worker processes used for data loading.\n",
    "- `val_split`: The proportion of the dataset reserved for validation.\n",
    "- `task`: Defines the type of task (e.g., 'multilabel' or 'multiclass').\n",
    "- `sample_rate`: The sample rate for audio data processing.\n",
    "- `class_weights_sampler`: Indicates whether to use class weights in the sampler for handling imbalanced datasets.\n",
    "- `class_limit`: The maximum number of samples per class.\n",
    "- `eventlimit`: Defines the maximum number of audio events processed per audio file, capping the quantity to ensure balance across files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2a0b4-c98c-4e95-9b51-3e7f17d038eb",
   "metadata": {},
   "source": [
    "#### Important Note:\n",
    "- The `class_weights_loss` parameter is currently deprecated and only implemented for focal loss. It's recommended to utilize the `class_weights_sampler` instead, as it has shown to yield favorable results, particularly as evidenced by the winner of the [BirdCLEF 2023](https://www.kaggle.com/competitions/birdclef-2023) challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58a9d4-aabd-43b7-b6f6-e7acbb4cefeb",
   "metadata": {},
   "source": [
    "Selecting appropriate values for these parameters is crucial, as they directly influence the efficiency of the training process and the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734f65a-8fb3-4468-8fa2-7bccff9f8097",
   "metadata": {},
   "source": [
    "### 2. Dataloader Configuration\n",
    "\n",
    "Once the dataset is configured, the next crucial step is setting up the data loaders. Data loaders are pivotal in efficiently feeding data into the model during both the training and testing phases. They manage the data flow, ensuring that the model is supplied with a consistent stream of data batches. In this section, we'll use the `LoaderConfig` and `LoadersConfig` classes to define different configurations for the training and testing data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a49150e-4ce1-4ed4-abc7-9d376eb9c3b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:55:43.941782600Z",
     "start_time": "2024-05-23T08:55:43.915891100Z"
    }
   },
   "outputs": [],
   "source": [
    "from birdset.datamodule import LoaderConfig, LoadersConfig\n",
    "\n",
    "# Configuration for the training data loader\n",
    "train_loader_config = LoaderConfig(\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    persistent_workers=False,\n",
    "    # prefetch_factor=None,\n",
    ")\n",
    "\n",
    "# Configuration for the testing data loader\n",
    "test_loader_config = LoaderConfig(\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False,\n",
    "    # prefetch_factor=None,\n",
    ")\n",
    "\n",
    "# Aggregating the loader configurations\n",
    "loaders_config = LoadersConfig(\n",
    "    train=train_loader_config,\n",
    "    valid=test_loader_config,\n",
    "    test=test_loader_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea42652-6857-4d0e-be29-8f50928aad0b",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `LoaderConfig` class:\n",
    "\n",
    "- `batch_size`: Specifies the number of samples contained in each batch. This is a crucial parameter as it impacts memory utilization and model performance.\n",
    "- `shuffle`: Determines whether the data is shuffled at the beginning of each epoch. Shuffling is typically used for training data to ensure model robustness and prevent overfitting.\n",
    "- `num_workers`: Sets the number of subprocesses to be used for data loading. More workers can speed up the data loading process but also increase memory consumption.\n",
    "- `pin_memory`: When set to `True`, enables the DataLoader to copy Tensors into CUDA pinned memory before returning them. This can lead to faster data transfer to CUDA-enabled GPUs.\n",
    "- `drop_last`: Determines whether to drop the last incomplete batch. Setting this to `True` is useful when the total size of the dataset is not divisible by the batch size.\n",
    "- `persistent_workers`: Indicates whether the data loader should keep the workers alive for the next epoch. This can improve performance at the cost of memory.\n",
    "- `prefetch_factor`: Defines the number of samples loaded in advance by each worker. This parameter is commented out here and can be adjusted based on specific requirements.\n",
    "\n",
    "Proper configuration of the data loaders is essential as it directly influences the efficiency of the training process, hardware resource utilization, and ultimately, the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a2390d-937a-46c8-8829-94ff3eca2b28",
   "metadata": {},
   "source": [
    "### 3. Configuration of TransformationsWrapper\n",
    "\n",
    "Transformations play a critical role in the data preparation process within the BirdSet data pipeline. These operations, applied before the data is fed into the model, encompass a range of augmentation techniques designed to regularize the model and prevent overfitting. Properly configured transformations not only enhance the diversity and quality of the training data but also help the model generalize better to new, unseen data.\n",
    "\n",
    "In the BirdSet framework, transformations are meticulously orchestrated through the `BirdSetTransformsWrapper` class. This wrapper acts as a comprehensive interface for defining and applying various transformations and augmentations to the data. It ensures that the data is consistently and effectively transformed, aligning with the specific requirements of the model and the inherent characteristics of the dataset.\n",
    "\n",
    "By configuring the `transforms_wrapper` using the `BirdSetTransformsWrapper` class, you gain precise control over how the data is manipulated during the preprocessing phase.\n",
    "\n",
    "To initialize the `BirdSetTransformsWrapper`, you need to provide the following parameters:\n",
    "\n",
    "```python\n",
    "from src.datamodule.components.transforms import BirdSetTransformsWrapper\n",
    "\n",
    "transforms = BirdSetTransformsWrapper(\n",
    "    task: Literal['multiclass', 'multilabel'] = \"multilabel\",\n",
    "    sample_rate: int = 32000,\n",
    "    model_type: Literal['vision', 'waveform'] = \"waveform\",\n",
    "    spectrogram_augmentations: DictConfig = DictConfig({}),\n",
    "    waveform_augmentations: DictConfig = DictConfig({}),\n",
    "    decoding: EventDecoding | None = None,\n",
    "    feature_extractor: DefaultFeatureExtractor = DefaultFeatureExtractor(),\n",
    "    max_length: int = 5,\n",
    "    nocall_sampler: DictConfig = DictConfig({}),\n",
    "    preprocessing: PreprocessingConfig = PreprocessingConfig()\n",
    ")\n",
    "```\n",
    "\n",
    "We will go through each of these parameters to understand their functionality and configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b8432",
   "metadata": {},
   "source": [
    "#### 3.1 Dataset and Model specific parameters\n",
    "The following parameters ensure that the transformations are tailored to the specific requirements of the dataset and the model:\n",
    "\n",
    "- `task`: Defines the type of task (e.g., 'multilabel' or 'multiclass').\n",
    "- `sample_rate`: The sample rate for audio data processing.\n",
    "- `model_type`: Specifies the type of model (e.g., 'vision' or 'waveform'). In case of a vison model, the input data is expected to be a spectrogram, while for a waveform model, the input data is the raw audio waveform.\n",
    "- max_length: The maximum length of the audio files in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16af9b0-b4da-47a3-a551-433009c4739d",
   "metadata": {},
   "source": [
    "#### 3.2 Augmentations\n",
    "\n",
    "Augmentations are powerful techniques applied to the data to introduce diversity and variability. They are particularly useful in audio and signal processing to enhance the robustness of models against variations in input data. In the BirdSet framework, you can configure waveform and spectrogram augmentations as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306dfe3d-b6a1-41e9-ab21-ac551e501dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Waveform Augmentations**\n",
    "\n",
    "These augmentations are applied directly to the audio waveform. In BirdSet, you can use any waveform augmentation technique as long as it can be composed by the [torch-audiomentations Compose function](https://github.com/asteroid-team/torch-audiomentations/blob/main/torch_audiomentations/core/composition.py). You can add waveform augmentations as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263b67d3-0a2f-4ccf-b8eb-d55af0236ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:55:48.164688100Z",
     "start_time": "2024-05-23T08:55:48.125622700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
      "  >>> augment = AddColoredNoise(..., output_type='dict')\n",
      "  >>> augmented_samples = augment(samples).samples\n",
      "  warnings.warn(\n",
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
      "  >>> augment = PitchShift(..., output_type='dict')\n",
      "  >>> augmented_samples = augment(samples).samples\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch_audiomentations import AddColoredNoise, PitchShift\n",
    "\n",
    "waveform_augmentation = {\n",
    "    \"colored_noise\": AddColoredNoise(\n",
    "        p=0.2, min_snr_in_db=3.0, max_snr_in_db=30.0, min_f_decay=-2.0, max_f_decay=2.0\n",
    "    ),\n",
    "    \"pitch_shift\": PitchShift(\n",
    "        p=0.2,\n",
    "        sample_rate=32000,\n",
    "        min_transpose_semitones=-4.0,\n",
    "        max_transpose_semitones=4.0,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c94eee-4cd7-49eb-b277-1902b48ced9e",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- `colored_noise`: Adds colored noise to the audio signal to simulate various real-world noise conditions.\n",
    "- `pitch_shift`: Alters the pitch of the audio signal, which is useful for simulating different tonal variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf77953-a58d-4e81-8e50-2d15be2ca0bf",
   "metadata": {},
   "source": [
    "**Spectrogram Augmentations**\n",
    "\n",
    "These augmentations are applied to the spectrogram representation of the audio. In BirdSet, you can use any spectrogram augmentation technique as long as it can be composed by the [torchvision Compose function](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html). You can add spectrogram augmentations as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58e9af4b-879b-4095-92da-99a5e0992c9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:55:50.790354600Z",
     "start_time": "2024-05-23T08:55:50.746233300Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomApply\n",
    "from torchaudio.transforms import TimeMasking, FrequencyMasking\n",
    "\n",
    "spectrogram_augmentations = {\n",
    "    \"time_masking\": RandomApply(\n",
    "        [TimeMasking(time_mask_param=100, iid_masks=True)], p=0.3\n",
    "    ),\n",
    "    \"frequency_masking\": RandomApply(\n",
    "        [FrequencyMasking(freq_mask_param=100, iid_masks=True)], p=0.5\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aca7d4-609d-4519-8675-ee7a35fa52c2",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- `time_masking`: Randomly masks a sequence of consecutive time steps in the spectrogram, helping the model become invariant to small temporal shifts.\n",
    "- `frequency_masking`: Randomly masks a sequence of consecutive frequency channels, encouraging the model to be robust against frequency variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04268897-f6f4-4488-9d85-12cf23c15baf",
   "metadata": {},
   "source": [
    "Configuring the augmentations correctly is crucial as they directly influence the model's ability to learn from a diverse set of data representations, ultimately leading to better generalization and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee00cb-31db-4f66-883f-623791389e06",
   "metadata": {},
   "source": [
    "#### 3.3 Decoding\n",
    "\n",
    "Decoding is a process, that converts the (compressed) data into a format that can be directly used by the model. In the BirdSet framework, we use the `EventDecoding` class by default. It is designed for preprocessing audio files in the context of event detection tasks. Its primary function is to ensure that each audio segment fed into the model is not only in the correct format, but also conditioned to improve the model's ability to identify and understand different audio events. Decoding is configured as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e2b022-ae03-4940-99de-3bf4823ce506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:55:53.127627600Z",
     "start_time": "2024-05-23T08:55:53.096907600Z"
    }
   },
   "outputs": [],
   "source": [
    "from birdset.datamodule.components import EventDecoding\n",
    "\n",
    "decoding = EventDecoding(\n",
    "    min_len=1.0,\n",
    "    max_len=5.0,\n",
    "    sample_rate=32000,\n",
    "    extension_time=8,\n",
    "    extracted_interval=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad60adc-3a7e-4506-9c7e-529074359b9f",
   "metadata": {},
   "source": [
    "Key Parameters:\n",
    "- `_target_`: Specifies the EventDecoding component to be used in the data processing pipeline.\n",
    "- `min_len` and `max_len`: Determine the minimum and maximum duration (in seconds) of the audio segments after decoding. These constraints ensure that each processed audio segment is of a suitable length for the model.\n",
    "- `sample_rate`: Defines the sample rate to which the audio should be resampled. This standardizes the input data's sample rate, making it consistent for model processing.\n",
    "- `extension_time`: Refers to the time (in seconds) by which the duration of an audio event is extended. This parameter is crucial for ensuring that shorter audio events are sufficiently long for the model to process effectively.\n",
    "- `extracted_interval`: Denotes the fixed duration (in seconds) of the audio segment that is randomly extracted from the extended audio event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6723e8-1b8c-4dcd-b55d-0e934f6f704f",
   "metadata": {},
   "source": [
    "Decoding is performed on the fly, ensuring that the data fed into the model is always in the correct format, even when the source data comes in various encoded forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af655211-4a1a-4015-b40f-a6f6fbf2647c",
   "metadata": {},
   "source": [
    "#### 3.4 Feature Extraction\n",
    "\n",
    "Feature extraction is a pivotal step in transforming raw data into a structured format that is suitable for model training. The `DefaultFeatureExtractor` in BirdSet is tailored for processing waveform data, providing a range of functionalities to prepare the data for model consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf56c70-dfc1-4bd4-8b91-a0e40ac06380",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:55:55.685254700Z",
     "start_time": "2024-05-23T08:55:55.665304600Z"
    }
   },
   "outputs": [],
   "source": [
    "from birdset.datamodule.components import DefaultFeatureExtractor\n",
    "\n",
    "feature_extractor = DefaultFeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sample_rate=32000,\n",
    "    padding_value=0.0,\n",
    "    return_attention_mask=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84eaf7-0d06-4a5d-91d1-55ede47349f5",
   "metadata": {},
   "source": [
    "Key Parameters:\n",
    "- `feature_size`: Determines the size of the extracted features.\n",
    "- `sample_rate`: The sample rate at which the audio data should be processed.\n",
    "- `padding_value`: The value used for padding shorter sequences to a consistent length.\n",
    "- `return_attention_mask`: Indicates whether an attention mask should be returned along with the processed features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae4db7-fa6a-4768-a00a-1e588d0534ac",
   "metadata": {},
   "source": [
    "This component is crucial for ensuring that the input data to the model is in a consistent and processable format, catering to models that require structured input in the form of PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98891b2a",
   "metadata": {},
   "source": [
    "#### 3.5 No-call Sampler\n",
    "You can use the `NoCallMixer` to add no-call samples to the dataset. This is particularly useful for training models to recognize the absence of bird calls. The `NoCallMixer` is configured as follows:\n",
    "```python\n",
    "from src.datamodule.components.no_call_sampler import NoCallSampler\n",
    "\n",
    "nocall_sampler = NoCallMixer(\n",
    "    directory: str = \"path/to/no_call_samples\",\n",
    "    p: float = 0.075,\n",
    "    sample_rate: int = 32000,\n",
    "    length: int = 5,\n",
    "    n_classes: int = 21,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2300978e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:55:58.507189300Z",
     "start_time": "2024-05-23T08:55:58.462284500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since this would require to have the dataset downloaded (see `download_background_noise.ipynb`, we will not use this for now\n",
    "nocall_sampler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a91ff-07b7-48f2-a115-7a15de770c52",
   "metadata": {},
   "source": [
    "#### 3.6 Configuration of Data Preprocessing\n",
    "\n",
    "Data preprocessing is a fundamental step in the BirdSet data pipeline, ensuring that the raw data is adequately conditioned and transformed, making it suitable for model consumption. The `PreprocessingConfig` class allows for a detailed specification of various preprocessing parameters, each carefully selected to meet the unique demands of your dataset and model. Here's how you can configure the data preprocessing in the BirdSet pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235ed383-b28f-4575-9a8b-02fff555e812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:56:00.635143200Z",
     "start_time": "2024-05-23T08:56:00.610853400Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Spectrogram\n",
    "from birdset.datamodule.components.resize import Resizer\n",
    "from birdset.datamodule.components.augmentations import PowerToDB\n",
    "from birdset.datamodule.components.transforms import PreprocessingConfig\n",
    "\n",
    "# Creating the preprocessing configuration\n",
    "preprocessing = PreprocessingConfig(\n",
    "    spectrogram_conversion=Spectrogram(\n",
    "        n_fft=1024,\n",
    "        hop_length=320,\n",
    "        power=2.0,\n",
    "    ),\n",
    "    resizer=Resizer(\n",
    "        db_scale=True,\n",
    "        target_height=None,\n",
    "        target_width=1024,\n",
    "    ),\n",
    "    dbscale_conversion=PowerToDB(),\n",
    "    normalize_spectrogram=True,\n",
    "    normalize_waveform=None,\n",
    "    mean=4.268,  # calculated on AudioSet\n",
    "    std=4.569,  # calculated on AudioSet\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff94aa-3191-41e8-a2ec-4393e8418c94",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `PreprocessingConfig` class:\n",
    "\n",
    "- `spectrogram_conversion`: This is an instance of the Spectrogram class from torchaudio.transforms. It is used to convert the audio waveform to a spectrogram. The parameters n_fft, hop_length, and power are used to configure the spectrogram conversion.\n",
    "\n",
    "    - `n_fft`: Th` size of the FFT, which will also determine the size of the window used for the STFT. It is set to 1024.\n",
    "    - `hop_length`: The number of samples between successive frames in the STFT. It is set to 320.\n",
    "    - `power`: The exponent for the magnitude spectrogram, e.g., 1 for energy, 2 for power, etc. It is set to 2.0.\n",
    "    - `resizer`: This is an instance of the Resizer class from src.datamodule.components.resize. It is used to resize the spectrogram. The parameters db_scale and target_width are used to configure the resizing.\n",
    "\n",
    "- `db_scale`: If set to True, the spectrogram is converted to dB scale. It is set to True.\n",
    "- `target_height`: The target height for the resized spectrogram. It is not set in this case.\n",
    "- target_width: The target width for the resized spectrogram. It is set to 1024.\n",
    "- `dbscale_conversion`: This is an instance of the PowerToDB class from src.datamodule.components.augmentations. It is used to convert the spectrogram to a dB scale.\n",
    "\n",
    "- `normalize_spectrogram`: If set to True, the spectrogram is normalized. It is set to True.\n",
    "\n",
    "- `normalize_waveform`: If set to a value, the audio waveform is normalized. It is not set in this case.\n",
    "\n",
    "- ``mean``: The mean value used for normalization. It is set to 4.268, which is calculated on AudioSet.\n",
    "\n",
    "- `std`: The standard deviation used for normalization. It is set to 4.569, which is calculated on AudioSet.\n",
    "\n",
    "\n",
    "By accurately configuring these preprocessing parameters, you ensure that the input data to the model is standardized and optimized for the learning process, which is essential for achieving high performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d993142c-54a2-43a0-9e4e-b977571b03fd",
   "metadata": {},
   "source": [
    "#### 3.7 Initiating the BirdSetTransformsWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "267876cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:56:03.756435400Z",
     "start_time": "2024-05-23T08:56:03.719469900Z"
    }
   },
   "outputs": [],
   "source": [
    "from birdset.datamodule.components.transforms import BirdSetTransformsWrapper\n",
    "\n",
    "transforms = BirdSetTransformsWrapper(\n",
    "    task=\"multilabel\",\n",
    "    sample_rate=32000,\n",
    "    model_type=\"vision\",\n",
    "    spectrogram_augmentations=spectrogram_augmentations,\n",
    "    waveform_augmentations=waveform_augmentation,\n",
    "    decoding=decoding,\n",
    "    feature_extractor=feature_extractor,\n",
    "    max_length=5,\n",
    "    nocall_sampler=nocall_sampler,\n",
    "    preprocessing=preprocessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb170e58-deed-4b52-ba18-160681b5dba0",
   "metadata": {},
   "source": [
    "### 4. Configuration of Event Mappings\n",
    "\n",
    "Event mapping plays a pivotal role in the data pipeline, serving as the bridge between raw dataset events and the structured input required by the model. This process ensures that each event in the dataset is accurately represented and can be effectively utilized during model training. By default, we use [bambird](https://www.sciencedirect.com/science/article/pii/S1574954122004022?casa_token=HEbcdB5MyRMAAAAA:saYbr1WNlJTs-kAZOtzMrNt5r1sN_69E7bMjfCJu2A4zlLLFoIt-5-Cht2Wryg59851H_PWgfHzw) for event mapping, which is implemented in the `XCEventMapping` class. Within the BirdSet framework, event mappings are configured as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4f30431-6972-4536-b323-88b7378e47c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:56:06.004675200Z",
     "start_time": "2024-05-23T08:56:05.954028700Z"
    }
   },
   "outputs": [],
   "source": [
    "from birdset.datamodule.components import XCEventMapping\n",
    "\n",
    "# Instantiate the event mapper\n",
    "mapper = XCEventMapping(\n",
    "    biggest_cluster=True,\n",
    "    no_call=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd405878-31de-4fbc-9729-20410747c89d",
   "metadata": {},
   "source": [
    "Key Parameters in Event Mapping:\n",
    "- `biggest_cluster`: If set to `True`, the mapper focuses on the biggest cluster of events, which can be particularly useful for datasets with imbalanced event distributions.\n",
    "- ``: Specifies the maximum number of events to consider. This can be used to limit the scope of the mapping, although it's usually already managed by the `DatasetConfig`.\n",
    "- `no_call`: Indicates whether 'no-call' events should be included. In this configuration, it's set to `False` as the no-call samples are handled separately by the `nocall_sampler`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc44f5-852c-4231-bafd-4757ee27841f",
   "metadata": {},
   "source": [
    "Properly configuring the event mappings is essential for ensuring that the model receives accurately structured and meaningful data, which is a cornerstone for effective model training and robust performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cfdb6-d25c-46b7-a1ee-47d4b834e01f",
   "metadata": {},
   "source": [
    "## Creating the BirdSet Datamodule\n",
    "\n",
    "The BirdSet Datamodule plays a central role in the BirdSet data pipeline, offering streamlined handling and preprocessing of BirdSet datasets to ensure they are primed for model training. Let's delve into the setup process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60180aa6-285c-4879-abca-e88b29578897",
   "metadata": {},
   "source": [
    "### Imports\n",
    "First, we import the necessary modules. `BirdSetDataModule` is responsible for managing the BirdSet datasets, while the `logging` module is used for logging information during the data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c76242c-928d-4404-922d-30c462681bb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:56:13.339917200Z",
     "start_time": "2024-05-23T08:56:13.290347900Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from birdset.datamodule.birdset_datamodule import BirdSetDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29444ab9-1b1c-4ea2-b85c-c55b280f2c16",
   "metadata": {},
   "source": [
    "### Creating Cache Directory\n",
    "The cache directory is a dedicated space for storing processed data. Utilizing a cache directory can significantly expedite subsequent data loading operations by avoiding redundant data processing. Here's how to create and manage a cache directory effectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c8b7b7a-2c44-40d1-a7db-1355d2aefe82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:56:15.351748900Z",
     "start_time": "2024-05-23T08:56:15.280666900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log the absolute path of the dataset\n",
    "logging.info(f\"Dataset path: <{os.path.abspath(dataset_config.data_dir)}>\")\n",
    "\n",
    "# Create the dataset directory if it does not exist\n",
    "os.makedirs(dataset_config.data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82895c4e-93ac-4590-aeb0-b0acb1f9909b",
   "metadata": {},
   "source": [
    "This approach ensures:\n",
    "- Organized data management: By maintaining a structured directory for your datasets, you facilitate easier access and management of your data assets.\n",
    "- Efficient data loading: By caching processed data, subsequent loads are much faster, which is particularly beneficial when working with large datasets.\n",
    "\n",
    "By carefully setting up the BirdSet Datamodule and managing your cache directory, you enhance the efficiency and reliability of your data pipeline, ensuring that your datasets are always ready for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764504c-f167-40b4-9297-6ca1184c5668",
   "metadata": {},
   "source": [
    "### Datamodule Initialization\n",
    "\n",
    "The `BirdSetDataModule` class plays a pivotal role in orchestrating the data pipeline. It consolidates the dataset configuration, data loaders, transformations, and event mappings into a cohesive structure, ensuring a clean and manageable workflow. Here's how the BirdSetDataModule is initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0586ee6-9cad-4270-acb9-931cf046b6ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:56:18.392557400Z",
     "start_time": "2024-05-23T08:56:18.370405800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the BirdSetDataModule\n",
    "datamodule = BirdSetDataModule(\n",
    "    dataset=dataset_config,\n",
    "    loaders=loaders_config,\n",
    "    transforms=transforms,\n",
    "    mapper=mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0982ec-199c-43bc-a2e6-8732f2356cc9",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `BirdSetDataModule` class:\n",
    "- `dataset`: The configuration settings for the dataset. It defines how the data is structured and managed.\n",
    "- `loaders`: Configuration settings for the data loaders, determining how data is batched and fed into the model.\n",
    "- `transforms`: The set of transformations and augmentations applied to the data, ensuring that it's properly conditioned for the model.\n",
    "- `mapper`: The event mapping configuration, essential for translating raw dataset events into a structured format that the model can interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab439e-2ee1-471f-9e38-15dfbc12cdfc",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The data preparation stage is where the actual data processing takes place. This stage is critical in ensuring that the data is correctly preprocessed, structured, and ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee400c19-41a3-4a5e-9011-509bdd319e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling: 100%|██████████| 21/21 [00:01<00:00, 10.66it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe35343da5045b2ba8d729c99976820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot-encoding train labels.:   0%|          | 0/17940 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/birdset/datamodule/base_datamodule.py:484: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5b62d00cfa423dbc166fec14e8a3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot-encoding test_5s labels.:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ec3274ae6c4f7ca2f0a524adfb0b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01c2aacc75b47278c591d023feaf919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6691c54d12fc4fe18dfd21137eec6b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9cf461-1fb8-4bb4-8db3-f7b4b4830036",
   "metadata": {},
   "source": [
    "The `prepare_data()` method encompasses various steps, including downloading the data (if not already locally available), applying the preprocessing steps defined in the transformations, and organizing the data into a format that is compatible with the model. It's a method that encapsulates the entire data preparation workflow, ensuring that the data is optimally prepared for the training process.\n",
    "\n",
    "This methodical approach to data preparation and modularization of the data pipeline components in the BirdSet framework contributes significantly to the efficiency, maintainability, and robustness of the machine learning lifecycle.\n",
    "\n",
    "**Hint**: If you recive an error concerning a not existing huggingface dataset, please make sure you are logged in to HuggingFace (see [Log in to Huggingface](#log-in-to-huggingface))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e3987-0f9a-4452-8b82-54c0ea96180e",
   "metadata": {},
   "source": [
    "### Datamodule Setup for Training Phase\n",
    "\n",
    "Setting up the datamodule for the training phase is a crucial step in the BirdSet data pipeline. This setup involves initializing the training and validation dataloaders, which play a vital role in supplying the model with data during the training process. The setup is performed using the `setup(stage=\"fit\")` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a252adb3-539e-434c-adcb-2ec2c1f7c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the datamodule for the training phase\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "# Retrieve the training and validation dataloaders\n",
    "train_loader = datamodule.train_dataloader()\n",
    "validation_loader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2048c240-bac0-465e-836f-1d9718c3a1ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:59:00.296838500Z",
     "start_time": "2024-05-23T08:58:58.460319100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n",
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n",
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n",
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n",
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n",
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n",
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n",
      "/workspace/birdset/datamodule/components/transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_values', 'labels']),\n",
       " torch.Size([32, 1, 128, 1024]),\n",
       " torch.Size([32, 21]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch a sample batch from the training dataloader\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Inspect the keys and shapes of the data in the batch\n",
    "batch.keys(), batch[\"input_values\"].shape, batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00066181-de6f-4980-90dc-458d6b92f27d",
   "metadata": {},
   "source": [
    "This code snippet demonstrates:\n",
    "- The initialization of the training phase.\n",
    "- The retrieval of training and validation dataloaders.\n",
    "- Fetching and inspecting a sample batch from the training dataloader.\n",
    "- The shapes of `input_values` and `labels` indicate the batch size, number of channels (if applicable), and dimensions of the input data and labels, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed731d9c-7aea-49ac-9df9-4fc61774abbc",
   "metadata": {},
   "source": [
    "### Datamodule Setup for Test Phase\n",
    "\n",
    "Similarly, the datamodule is set up for the test phase to ensure that the model can be effectively evaluated using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5db95322-ca2b-4f8f-b419-40529d7c92b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:59:05.646911900Z",
     "start_time": "2024-05-23T08:59:05.565471400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup the datamodule for the test phase\n",
    "datamodule.setup(stage=\"test\")\n",
    "\n",
    "# Retrieve the test dataloader\n",
    "test_loader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ed904-f95b-4bad-a03b-531de0a67dc7",
   "metadata": {},
   "source": [
    "The `setup(stage=\"test\")` method prepares the datamodule specifically for the test phase, and `test_dataloader()` retrieves the test dataloader, which is instrumental for batching and loading the test data efficiently during the model evaluation process.\n",
    "\n",
    "By methodically setting up the datamodule for both training and test phases, you ensure that the model has access to well-prepared data, which is essential for accurate training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567af44-494a-4547-b57b-9b487e57a520",
   "metadata": {},
   "source": [
    "### Usage in TensorFlow\n",
    "\n",
    "Utilizing the BirdSet datamodule in a TensorFlow environment involves integrating the prepared dataloaders with TensorFlow's training and evaluation workflows. This integration ensures that the data is fed into TensorFlow models efficiently and in a format that TensorFlow can process. Here's how you can set up the BirdSet datamodule for TensorFlow compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce1c99c0-201a-4cae-9e9e-d4c6f9512fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:59:08.064567700Z",
     "start_time": "2024-05-23T08:59:07.871744300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup the datamodule for the training phase\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "# Retrieve the training and validation datasets\n",
    "train_loader = datamodule.train_dataset\n",
    "validation_loader = datamodule.val_dataset\n",
    "\n",
    "# Setup the datamodule for the test phase\n",
    "datamodule.setup(stage=\"test\")\n",
    "\n",
    "# Retrieve the test dataset\n",
    "test_loader = datamodule.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bda355-34f2-42b2-b57a-b6112f48f49c",
   "metadata": {},
   "source": [
    "#### Key Considerations:\n",
    "- `train_dataset`, `validation_dataset`, and `test_dataset` are the datasets prepared by the BirdSet datamodule, ready to be used in TensorFlow's training and evaluation routines.\n",
    "- It's important to ensure that these datasets are in a format compatible with TensorFlow. This might involve additional steps such as converting the data to `tf.data.Dataset` objects or applying necessary transformations to align with TensorFlow's data handling mechanisms.\n",
    "- More information on this integration process can be found in [HuggingFace's documentation](https://huggingface.co/docs/datasets/use_with_tensorflow).\n",
    "\n",
    "By following these steps, you can leverage the robust data preprocessing and management capabilities of the BirdSet datamodule within a TensorFlow environment, facilitating an efficient and streamlined model training and evaluation process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdset-xS3fZVNL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
